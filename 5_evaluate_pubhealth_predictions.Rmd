---
title: "5_gephi_bec"
author: "Prashant Garg"
date: "2024-06-06"
output: html_document
---

#init
```{r}
library(data.table)
library(magrittr)
library(jsonlite)
library(tidyverse)
library(ggrepel)
library(jsonlite)
```

# data

metadata
```{r}
con <- file("int_data/PUBHEALTH_processed_health_claims.json", "r")
df <- stream_in(con, flatten = TRUE)
close(con)

```

responses
```{r}
# Load required packages
library(jsonlite)

# 1. List all JSON files in the folder that start with "responses_"
json_files <- list.files(
  path = "int_data/responses_PUBHEALTH/",
  pattern = "^responses.*\\.json$",
  full.names = TRUE
)

# 2. Read each JSON into a data frame
dfs <- lapply(json_files, function(file) {
  # Assumes each JSON file is an array of records (list of objects)
  fromJSON(file, flatten = TRUE)
})

# 3. Combine all data frames into one
df_llm <- bind_rows(dfs)
rm(dfs)
```


```{r}
setDT(df_llm)
# 1. Split off the model and iteration (by “__”)
df_llm[, c("prefix", "model", "iteration") := tstrsplit(custom_id, "__", fixed = TRUE)]

# 2. Split the prefix into the “claim” literal, claim_id, and language (by “_”)
df_llm[, c("dummy", "claim_id") := tstrsplit(prefix, "_", fixed = TRUE)]

# 3. Clean up helper columns and convert iteration to integer
df_llm[, `:=`(
  prefix = NULL,
  dummy = NULL,
  iteration = as.integer(iteration)
)]
```

clean output
```{r}
df_llm %<>% rename(response_raw=response)


df_llm[, response := {
  # Helper: for a single string, extract all '0' or '1' characters and return the last as integer
  extract_last01 <- function(txt) {
    # find all matches of the characters '0' or '1'
    matches <- regmatches(txt, gregexpr("[01]", txt))[[1]]
    if (length(matches) > 0) {
      return(as.integer(tail(matches, 1)))
    } else {
      return(NA_integer_)
    }
  }
  # Vectorise over the column
  vapply(response_raw, extract_last01, integer(1))
}]

# Check the distribution
a<-df_llm[, .N, by = response][order(response)]

df_llm[,response:=ifelse(response>1,NA,response)]
```

join metadata
```{r}
df_llm %<>% left_join(df, by="claim_id")
```

# evaluation
```{r}
setDT(df_llm)

# 1) (Re)create the binary truth
df_llm[ , true_label := fifelse(
     label == "true",  1L,
fifelse(label == "false", 0L, NA_integer_)
)]

# 2) Keep only the definitive cases
eval_dt <- df_llm[!is.na(true_label) & !is.na(response)]

#  Metrics on the “modal” prediction per claim_id & model
#    (i.e. take the most common response across the 3 runs)

# a) collapse to one row per (model, claim_id) with modal_response
modal_dt <- eval_dt[ , {
    tb <- table(response)
    modal_resp <- as.integer(names(tb)[which.max(tb)])
    .(modal_response = modal_resp,
      true_label    = unique(true_label))  # same for all iterations
}, by = .(model, claim_id)]

# b) compute confusion counts by model
metrics_modal <- modal_dt[ , .(
  TP = sum(modal_response == 1L & true_label == 1L),
  TN = sum(modal_response == 0L & true_label == 0L),
  FP = sum(modal_response == 1L & true_label == 0L),
  FN = sum(modal_response == 0L & true_label == 1L)
), by = model]

metrics_modal[ , c("precision","recall","f1","accuracy") := {
    prec <- TP / (TP + FP)
    rec  <- TP / (TP + FN)
    acc  <- (TP + TN) / (TP + TN + FP + FN)
    f1v  <- 2 * prec * rec / (prec + rec)
    list(prec, rec, f1v, acc)
} ]

# ——————————————————————
# All
# ——————————————————————
All_df <- metrics_modal %>%
  mutate(
    model_clean = model %>%
      str_remove_all("-\\d{4}-\\d{2}-\\d{2}") %>%
      str_remove_all(":latest"),
    category    = "All",
    accuracy    = accuracy
  ) %>%
  select(model_clean, category, accuracy)

# ——————————————————————
# Topics
# ——————————————————————
# Re‐create & recode your subjects
claim_meta2 <- eval_dt %>%
  distinct(claim_id, subjects) %>%
  separate_rows(subjects, sep = ",\\s*") %>%
  mutate(
    subject = tolower(str_trim(subjects)),
    subject = case_when(
      str_detect(subject, regex("covid|coronavirus|virus outbreak|epidemic|flu|measles|ebola")) ~ "covid-19",
      str_detect(subject, regex("news"))    ~ "news",
      str_detect(subject, regex("health|healthcare|health care")) ~ "health",
      str_detect(subject, regex("politics")) ~ "politics",
      str_detect(subject, regex("abortion")) ~ "abortion",
      TRUE ~ subject
    )
  ) %>%
  filter(subject %in% c("abortion","health","politics","news","covid-19"))

# Modal predictions per claim
modal_pred <- eval_dt %>%
  filter(!is.na(true_label)) %>%
  group_by(model, claim_id) %>%
  summarize(
    modal_response = as.integer(names(which.max(table(response)))),
    true_label     = first(true_label),
    .groups = "drop"
  ) %>%
  mutate(
    model_clean = model %>%
      str_remove_all("-\\d{4}-\\d{2}-\\d{2}") %>%
      str_remove_all(":latest")
  )

# Join + compute topic accuracy
topic_df <- modal_pred %>%
  inner_join(claim_meta2, by = "claim_id") %>%
  group_by(model_clean, subject) %>%
  summarize(
    accuracy = mean(modal_response == true_label),
    .groups = "drop"
  ) %>%
  rename(category = subject)   # ← critical!

# ——————————————————————
# Sources
# ——————————————————————

# 1. Extract top‐level domains from the comma‐separated `sources` URLs
domains_df <- eval_dt %>%
  select(claim_id, sources) %>%
  distinct() %>%
  separate_rows(sources, sep = ",\\s*") %>%
  filter(!is.na(sources), sources != "") %>%
  mutate(
    # strip protocol + “www.”, then grab up to first slash
    domain = sources %>%
      str_remove("^https?://") %>%
      str_remove("^www\\.") %>%
      str_extract("^[^/]+")
  )

# 2. Instead of auto-counting, manually specify your domains of interest:
top10_domains <- c(
  "nature.com","foxnews.com","nytimes.com","youtube.com","cdc.gov"
)

# 3. Compute each model’s modal prediction per claim_id
modal_pred <- eval_dt %>%
  filter(!is.na(true_label)) %>%        # only keep rows with a true_label
  group_by(model, claim_id) %>%
  summarize(
    modal_response = as.integer(names(which.max(table(response)))),
    true_label     = first(true_label),
    .groups = "drop"
  )

# 4. Join in only the top‐10 domains
modal_domains <- modal_pred %>%
  inner_join(
    domains_df %>% filter(domain %in% top10_domains),
    by = "claim_id"
  )

# 5. Clean up model names for display
modal_domains <- modal_domains %>%
  mutate(
    model_clean = model %>%
      str_remove_all("-\\d{4}-\\d{2}-\\d{2}") %>%
      str_remove_all(":latest")
  )

# 6. Compute modal accuracy by model × domain
domain_metrics <- modal_domains %>%
  group_by(model_clean, domain) %>%
  summarize(
    accuracy = mean(modal_response == true_label),
    .groups = "drop"
  )

source_df <- domain_metrics %>%
  filter(domain %in% c("nature.com","foxnews.com","nytimes.com","youtube.com","cdc.gov")) %>%
  rename(
    category    = domain
  ) %>%
  select(model_clean, category, accuracy)

# ——————————————————————
# Bind & Plot
# ——————————————————————
heatmap_df <- bind_rows(All_df, topic_df, source_df) %>%
  mutate(
    category = factor(
      category,
      levels = c(
        "All",
        "abortion","health","politics","news","covid-19",
        "nature.com","foxnews.com","nytimes.com","youtube.com","cdc.gov"
      )
    ),
    inaccuracy = 1 - accuracy
  ) %>%
  # reorder both axes by mean inaccuracy
  mutate(
    category    = fct_reorder(category,    inaccuracy, .fun = mean, .desc = FALSE),
    model_clean = fct_reorder(model_clean, inaccuracy, .fun = mean, .desc = FALSE)
  )


heatmap_inacc <- heatmap_df %>%
  mutate(
    inaccuracy   = 1 - accuracy,
    panel        = case_when(
      category == "All"                                                      ~ "All",
      category %in% c("abortion","health","politics","news","covid-19")           ~ "Topics",
      category %in% c("nature.com","foxnews.com","nytimes.com","youtube.com","cdc.gov") ~ "Sources",
      TRUE                                                                       ~ NA_character_
    ),
    model_clean = str_replace_all(model_clean,
                                  "gemini-2\\.0-flash-lite", "gemini-2-flash")
  ) %>%
  group_by(model_clean) %>%
  mutate(global_inacc = mean(inaccuracy, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(model_clean = fct_reorder(model_clean, global_inacc, .desc = FALSE)) %>%
  group_by(panel) %>%
  mutate(category = fct_reorder(category, inaccuracy, .desc = TRUE)) %>%
  ungroup()

ggplot(heatmap_inacc, aes(x = category, y = model_clean, fill = inaccuracy)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "indianred", name = "Inaccuracy") +
  facet_grid(. ~ panel, scales = "free_x", space = "free_x") +
  labs(x = "Categories (most inaccurate →)", y = "Model (most inaccurate →)") +
  theme_classic(base_size = 20) +
  theme(
    strip.text        = element_text(face = "bold"),
    panel.spacing.x   = unit(0.5, "lines"),
    axis.text.x       = element_text(angle = 45, hjust = 1),
    # axis.text.y       = element_text(size = 10)
  )

ggsave(plot=last_plot(), filename="figures/fig2.jpg",width = 12,height = 8,dpi=300)
ggsave(plot=last_plot(), filename="figures/fig2.pdf",width = 12,height = 8,dpi=300)

```

# Supplementary Datasets

Dataset S1 — Fixed‑effect logit coefficients
```{r}
library(data.table)   # fast tables
library(fixest)       # feglm()
library(broom)        # tidy()

## 0 ── make sure the three inputs are data.tables
setDT(modal_dt)      # already contains: model, claim_id, modal_response, true_label
setDT(claim_meta2)   # claim_id  |  subject         (from your earlier recode)
setDT(domains_df)    # claim_id  |  domain          (top‑level URL domains)

## 1 ── add correctness flag
modal_dt[ , correct := as.integer(modal_response == true_label)]

## 2 ── attach a single canonical topic per claim
topic_lookup  <- claim_meta2[ , .(topic  = first(subject)), by = claim_id]

## 3 ── attach a primary source domain per claim
source_lookup <- domains_df[ , .(source = first(domain)) , by = claim_id]

## 4 ── merge look‑ups  →  long format
modal_long <- modal_dt[
  topic_lookup , on = "claim_id"][
  source_lookup, on = "claim_id"]

## 5 ── keep only the categories that appear in Figure 2
keep_topics  <- c("abortion","health","politics","news","covid-19")
keep_sources <- c("nature.com","foxnews.com","nytimes.com",
                  "youtube.com","cdc.gov")

modal_long_trim <- copy(modal_long)[
  , topic  := fcase(topic  %in% keep_topics , topic , default = "Other")
][
  , source := fcase(source %in% keep_sources, source, default = "Other")
]

## drop rows that still have missing critical fields
modal_long_trim <- na.omit(
  modal_long_trim,
  cols = c("correct","model","topic","source")
)

## 6 ── claim‑level fixed‑effect logit (clustered SEs)
fe_fit <- feglm(
  correct ~ model + topic + source | claim_id,
  data    = modal_long_trim,
  family  = binomial("logit")
)

table_S1 <- broom::tidy(
  fe_fit,
  conf.int   = TRUE,
  conf.level = 0.95,
  cluster    = "claim_id"
)

## 7 ── save to Supplementary Dataset S1
dir.create("supplementary_datasets", showWarnings = FALSE)
data.table::fwrite(
  table_S1,
  file = "supplementary_datasets/dataset_S1.csv"
)

```

Dataset S2 — Pairwise McNemar tests (BH‑adjusted)
```{r}
## ── packages ───────────────────────────────────────────
library(data.table)
library(DescTools)    # mcnemar.test
library(purrr)        # map_dfr()

## ── wide 0/1 correctness matrix per claim ─────────────
wide_mat <- dcast(
  modal_long_trim,
  claim_id ~ model,
  value.var = "correct"
)

models <- setdiff(names(wide_mat), "claim_id")

## McNemar on every unordered pair
run_mcnemar <- function(a, b) {
  tab <- table(wide_mat[[a]], wide_mat[[b]])
  out <- mcnemar.test(tab, correct = TRUE)
  data.frame(model_A  = a,
             model_B  = b,
             statistic = out$statistic,
             p_value   = out$p.value)
}

S2 <- map_dfr(combn(models, 2, simplify = FALSE),
              \(m) run_mcnemar(m[1], m[2]))

S2$p_adj <- p.adjust(S2$p_value, method = "BH")
data.table::setorder(S2, p_adj)

data.table::fwrite(S2, "supplementary_datasets/dataset_S2.csv")

```

Dataset S3 — Readability (F‑K grade) mixed‑logit
```{r}
## ── packages ───────────────────────────────────────────
library(data.table)
library(quanteda.textstats)   # textstat_readability()
library(fixest)               # feglm()
library(broom)                # tidy()

## ── 0.  F‑K grade per claim  ───────────────────────────
setDT(df)   # df already has claim_id + claim text
df[ , fk_grade :=
      textstat_readability(claim,
                           measure = "Flesch.Kincaid")$Flesch.Kincaid ]

## ── 1.  merge into trimmed analysis table ──────────────
modal_enh <- modal_long_trim[         # produced in Dataset S1 code
  df[ , .(claim_id, fk_grade)],
  on = "claim_id"
]

## ── 2.  logit without claim FE (clustered by claim) ───
fe_noclaim <- feglm(
  correct ~ model + topic + source + fk_grade,
  data    = modal_enh,
  family  = binomial("logit"),
  cluster = "claim_id"         # robust SEs
)

## ── 3.  tidy + save as Dataset S3 ──────────────────────
table_S3 <- broom::tidy(
  fe_noclaim,
  conf.int   = TRUE,
  conf.level = 0.95,
  cluster    = "claim_id"
)

data.table::fwrite(
  table_S3,
  file = "supplementary_datasets/dataset_S3.csv"
)


```

