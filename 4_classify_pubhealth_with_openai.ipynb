{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path and sheet name\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file paths for the TSV files\n",
    "files = [\"data/PUBHEALTH/train.tsv\", \"data/PUBHEALTH/dev.tsv\", \"data/PUBHEALTH/test.tsv\"]\n",
    "\n",
    "# Read each TSV file into a DataFrame using list comprehension\n",
    "dfs = [pd.read_csv(file, sep='\\t') for file in files]\n",
    "\n",
    "# Combine the DataFrames into one, resetting the index\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique claim_id in df\n",
    "print(df['claim_id'].nunique())\n",
    "\n",
    "# filter df to keep only unique claim_id\n",
    "df = df.drop_duplicates(subset='claim_id')\n",
    "\n",
    "# reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(df['claim_id'].nunique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out cases where claim_id is empty in df\n",
    "df = df[df['claim_id'].notna()]\n",
    "\n",
    "# filter out cases where claim id is longer than 100 characters (cleanup)\n",
    "df = df[df['claim_id'].str.len() <= 512]\n",
    "\n",
    "# reset index\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save df as .json file\n",
    "df.to_json(\"int_data/PUBHEALTH_processed_health_claims.json\", orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read df from json file\n",
    "df_grouped = pd.read_json(\"int_data/PUBHEALTH_processed_health_claims.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "# Make sure df is already defined (e.g. via pandas.read_csv or similar)\n",
    "# from openai import OpenAI  <-- imported inside processing functions as needed\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# English-specific text\n",
    "# ------------------------------------------------------------------------------\n",
    "system_instructions = (\n",
    "    \"You are an expert in evaluating medical claims. Your task is to assess the truthfulness of the provided health claim \"\n",
    "    \"using established medical research, guidelines, and evidence. Determine if the claim is true. If the claim is supported by evidence, \"\n",
    "    \"respond with '1'. If the claim is contradicted by evidence, respond with '0'. If you are unsure, choose the option that is closest to the available evidence. \"\n",
    ")\n",
    "\n",
    "strict_response_line = \"Your answer must be exactly one of the following: '1' or '0'. Do not include any additional text.\"\n",
    "\n",
    "prompt_labels = {\n",
    "    \"claim\": \"Claim:\"\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Function to create a combined prompt entry for a given claim\n",
    "# ------------------------------------------------------------------------------\n",
    "def create_prompt_entry(claim_id, claim,  model, iteration):\n",
    "    # Build the combined prompt:\n",
    "    # 1. The original system instruction\n",
    "    # 2. The claim details using fixed prompt labels\n",
    "    # 3. The strict instruction in English\n",
    "    prompt_text = (\n",
    "        f\"{system_instructions}\\n\\n\"\n",
    "        f\"{prompt_labels['claim']} {claim}\\n\"\n",
    "        f\"{strict_response_line}\"\n",
    "    )\n",
    "    # To mimic the template's structure, we include a dummy field (\"NA\") as the second part\n",
    "    custom_id = f\"claim_{claim_id}__{model}__{iteration}\"\n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"prompt\": prompt_text\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Build the list of prompt entries across models and iterations\n",
    "# ------------------------------------------------------------------------------\n",
    "prompts = []\n",
    "models = [\n",
    "    \"gpt-4o-mini-2024-07-18\"\n",
    "]\n",
    "iterations = range(3)  # e.g., 0, 1, 2\n",
    "\n",
    "for model in models:\n",
    "    for _, row in df.iterrows():\n",
    "        for iteration in iterations:\n",
    "            entry = create_prompt_entry(\n",
    "                row['claim_id'],\n",
    "                row['claim'],\n",
    "                model,\n",
    "                iteration\n",
    "            )\n",
    "            prompts.append(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"KEY\" \n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "def call_openai(entry):\n",
    "    custom_id = entry[\"custom_id\"]\n",
    "    prompt    = entry[\"prompt\"]\n",
    "    model     = custom_id.split(\"__\")[1]\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model      = model,\n",
    "            messages   = [{\"role\":\"user\",\"content\":prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens =50\n",
    "        )\n",
    "        return {\n",
    "            \"custom_id\": custom_id,\n",
    "            \"model\":     model,\n",
    "            \"response\":  resp.choices[0].message.content.strip()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"custom_id\": custom_id,\n",
    "            \"model\":     model,\n",
    "            \"error\":     str(e)\n",
    "        }\n",
    "\n",
    "responses = []\n",
    "# choose a worker count based on your bandwidth & rateâ€limit headroom\n",
    "with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "    futures = [executor.submit(call_openai, p) for p in prompts]\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Parallelized\"):\n",
    "        responses.append(future.result())\n",
    "\n",
    "with open(\"int_data/PUBHEALTH_all_responses.json\", \"w\", encoding=\"utf-8\") as out:\n",
    "    json.dump(responses, out, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
