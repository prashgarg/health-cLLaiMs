{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 0. Imports & paths\n",
    "# ---------------------------------------------------------------------------\n",
    "import os, json, random, time, pickle, logging\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI          # pip install openai >=1.14\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Load forward-translation dataframe\n",
    "# ---------------------------------------------------------------------------\n",
    "df_trans = pd.read_csv(\n",
    "    \"int_data/UK_extracted_hc_multilingual.csv\",  # claim_id | language | translation_*\n",
    "    dtype={\"claim_id\": int, \"language\": str}\n",
    ")\n",
    "print(f\"Loaded {len(df_trans):,} translated rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2.  GPT‑4o‑mini back‑translation template  (multilingual → English)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# --- strict JSON schema for the model’s answer ------------------------------\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"claim_backtranslation_v1\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"english_claim\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Health‑claim text translated into UK English.\"\n",
    "                },\n",
    "                \"english_nutrient_substance\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Nutrient / food item translated into UK English.\"\n",
    "                },\n",
    "                \"english_health_relationship\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Health relationship translated into UK English.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"english_claim\",\n",
    "                \"english_nutrient_substance\",\n",
    "                \"english_health_relationship\"\n",
    "            ],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- system instruction (mirrors forward‑translation wording) ---------------\n",
    "system_instruction = (\n",
    "    \"You are an expert translator specializing in medical claims. \"\n",
    "    \"Your task is to accurately translate the provided text from the source \"\n",
    "    \"language specified in the user prompt into English. \"\n",
    "    \"This text includes three parts: the health claim, the nutrient substance \"\n",
    "    \"(or food/food category), and the health relationship. \"\n",
    "    \"Please provide your translations using the keys \"\n",
    "    \"'english_claim', 'english_nutrient_substance', and \"\n",
    "    \"'english_health_relationship'. \"\n",
    "    \"Be precise and maintain the original meaning. If a specialized medical \"\n",
    "    \"term does not have an established English equivalent, keep the original \"\n",
    "    \"term from the source language. \"\n",
    "    \"Your response must strictly adhere to the JSON schema provided in the \"\n",
    "    \"response format.\"\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Helper to build one JSONL entry per row\n",
    "# ---------------------------------------------------------------------------\n",
    "def make_jsonl_entry(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Build one JSONL line for the OpenAI batch endpoint.\n",
    "    custom_id layout: 'backtrans_<claim_id>_<lang>'\n",
    "    \"\"\"\n",
    "    custom_id = f\"backtrans_{row.claim_id}_{row.language}\"\n",
    "    \n",
    "    user_prompt = (\n",
    "        f\"Source language: '{row.language}'. Translate the following into English:\\n\"\n",
    "        f\"translation_claim: {row.translation_claim}\\n\"\n",
    "        f\"translation_nutrient_substance: {row.translation_nutrient_substance}\\n\"\n",
    "        f\"translation_health_relationship: {row.translation_health_relationship}\"\n",
    "    )\n",
    "\n",
    "    payload = {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_instruction},\n",
    "                {\"role\": \"user\",   \"content\": user_prompt}\n",
    "            ],\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 10000,\n",
    "            \"response_format\": response_format\n",
    "        }\n",
    "    }\n",
    "    return json.dumps(payload, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Write master JSONL file\n",
    "# ---------------------------------------------------------------------------\n",
    "master_jsonl = \"int_data/back_translate/openai/batch_input_hc_backtranslation.jsonl\"\n",
    "with open(master_jsonl, \"w\", encoding=\"utf-8\") as fh:\n",
    "    for _, r in df_trans.iterrows():\n",
    "        fh.write(make_jsonl_entry(r) + \"\\n\")\n",
    "print(f\"Master JSONL written: {master_jsonl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. Split into ≤N shards (keeps each file comfortably <100 MB)\n",
    "# ---------------------------------------------------------------------------\n",
    "def split_jsonl(source, out_dir, n_parts=4):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    with open(source, \"r\", encoding=\"utf-8\") as fh:\n",
    "        lines = fh.readlines()\n",
    "\n",
    "    chunk = (len(lines) + n_parts - 1) // n_parts\n",
    "    for i in range(n_parts):\n",
    "        part_lines = lines[i*chunk : (i+1)*chunk]\n",
    "        if not part_lines: break\n",
    "        tgt = f\"{out_dir}/backtranslation_part{i+1}.jsonl\"\n",
    "        with open(tgt, \"w\", encoding=\"utf-8\") as out:\n",
    "            out.writelines(part_lines)\n",
    "        print(f\"  wrote {len(part_lines):,} lines → {tgt}\")\n",
    "\n",
    "split_jsonl(master_jsonl,\n",
    "            \"int_data/back_translate/openai/input_batches_hc_backtranslation\",\n",
    "            n_parts=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"KEY\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5. Helper: upload & create batch jobs (commented – run when ready)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "client = OpenAI()                        # reads OPENAI_API_KEY from env\n",
    "\n",
    "def upload_and_launch(jsonl_path, desc):\n",
    "    infile = client.files.create(file=open(jsonl_path,\"rb\"), purpose=\"batch\")\n",
    "    batch  = client.batches.create(\n",
    "        input_file_id=infile.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"description\": desc}\n",
    "    )\n",
    "    print(f\"Launched batch {batch.id} for {jsonl_path}\")\n",
    "    return batch.id\n",
    "\n",
    "batch_ids = []\n",
    "for fp in sorted(os.listdir(\"int_data/back_translate/openai/input_batches_hc_backtranslation\")):\n",
    "    if fp.endswith(\".jsonl\"):\n",
    "        full = f\"int_data/back_translate/openai/input_batches_hc_backtranslation/{fp}\"\n",
    "        batch_ids.append(upload_and_launch(full, f\"Back-translation {fp}\"))\n",
    "# Save for later polling\n",
    "pickle.dump(batch_ids, open(\"int_data/back_translate/openai/batch_ids_hc_backtranslation.pkl\",\"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 6. Helper: poll batches & collect outputs (run after 24 h window)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "out_dir = \"int_data/back_translate/openai/output_batches_hc_backtranslation\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for bid in batch_ids:\n",
    "    b = client.batches.retrieve(bid)\n",
    "    if b.status != \"completed\":\n",
    "        print(f\"{bid} → {b.status}\")\n",
    "        continue\n",
    "    out_id = b.output_file_id\n",
    "    outfile_resp = client.files.content(out_id)\n",
    "    out_path = f\"{out_dir}/backtrans_output_{bid}.jsonl\"\n",
    "    with open(out_path,\"w\",encoding=\"utf-8\") as fh:\n",
    "        fh.write(outfile_resp.text)\n",
    "    print(f\"Saved {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 7. Parse outputs into a tidy DataFrame (optional)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def load_jsonl(fp):\n",
    "    with open(fp,\"r\",encoding=\"utf-8\") as fh:\n",
    "        return [json.loads(l) for l in fh]\n",
    "\n",
    "records = []\n",
    "for fp in os.listdir(out_dir):\n",
    "    if fp.endswith(\".jsonl\"):\n",
    "        for entry in load_jsonl(f\"{out_dir}/{fp}\"):\n",
    "            cid = entry[\"custom_id\"]\n",
    "            if (\"response\" in entry and\n",
    "                \"body\" in entry[\"response\"] and\n",
    "                \"choices\" in entry[\"response\"][\"body\"]):\n",
    "                raw = entry[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "                try:\n",
    "                    js = json.loads(raw)\n",
    "                except json.JSONDecodeError:\n",
    "                    logging.warning(f\"Bad JSON for {cid}\")\n",
    "                    continue\n",
    "                claim_id, lang = cid.split(\"_\")[1:3]\n",
    "                records.append({\n",
    "                    \"claim_id\": int(claim_id),\n",
    "                    \"language\": lang,\n",
    "                    **js\n",
    "                })\n",
    "\n",
    "df_back = pd.DataFrame(records)\n",
    "df_back.to_csv(\"int_data/back_translate/openai/backtranslations_en.csv\", index=False)\n",
    "print(\"✅ back-translation dataframe saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 8. Merge forward‑ and back‑translations\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- reload the two CSVs -----------------------------------------------------\n",
    "df_forward = pd.read_csv(\n",
    "    \"int_data/multilingual/UK_extracted_hc_multilingual.csv\",   # forward translations\n",
    "    dtype={\"claim_id\": int, \"language\": str}\n",
    ")\n",
    "\n",
    "df_back = pd.read_csv(\n",
    "    \"int_data/back_translate/openai/UK_backtranslations_en.csv\",       # back‑translations\n",
    "    dtype={\"claim_id\": int, \"language\": str}\n",
    ")\n",
    "\n",
    "print(f\"Forward rows : {len(df_forward):,}\")\n",
    "print(f\"Back rows    : {len(df_back):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- sanity‑check keys -------------------------------------------------------\n",
    "missing = set(df_back[\"claim_id\"]) - set(df_forward[\"claim_id\"])\n",
    "if missing:\n",
    "    print(f\"⚠️  Warning: {len(missing)} claim_id(s) in back‑translations not found in forward table.\")\n",
    "\n",
    "# --- merge ------------------------------------------------------------------\n",
    "df_merged = (\n",
    "    df_forward\n",
    "    .merge(\n",
    "        df_back,\n",
    "        on=[\"claim_id\", \"language\"],\n",
    "        how=\"left\",\n",
    "        validate=\"one_to_one\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Merged rows  : {len(df_merged):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- save -------------------------------------------------------------------\n",
    "out_path = \"int_data/back_translate/openai/UK_merged_translations.csv\"\n",
    "df_merged.to_csv(out_path, index=False)\n",
    "print(f\"✅ Merged file written → {out_path}\")\n",
    "\n",
    "# --- optional peek ----------------------------------------------------------\n",
    "print(df_merged.head(3).T)   # transpose for compact view\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"KEY\" \n",
    "\n",
    "# pip install langchain_openai\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_3072 = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 0️⃣  Imports & configs\n",
    "# ------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1️⃣  Load merged translations\n",
    "# ------------------------------------------------------------------\n",
    "df = pd.read_csv(\"int_data/back_translate/openai/UK_merged_translations.csv\",\n",
    "                 dtype={\"claim_id\": int, \"language\": str})\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2️⃣  Embed the two text columns in one go\n",
    "# ------------------------------------------------------------------\n",
    "tqdm.pandas(desc=\"Embedding original EN claims\")\n",
    "claim_embeddings = np.vstack(\n",
    "    embeddings_3072.embed_documents(df[\"Claim\"].astype(str).tolist())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tqdm.pandas(desc=\"Embedding back‑translations\")\n",
    "back_embeddings  = np.vstack(\n",
    "    embeddings_3072.embed_documents(df[\"english_claim\"].astype(str).tolist())\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3️⃣  Cosine similarity row‑wise (vectorised, no loops)\n",
    "# ------------------------------------------------------------------\n",
    "numerators = np.einsum(\"ij,ij->i\", claim_embeddings, back_embeddings)\n",
    "denom      = (\n",
    "    np.linalg.norm(claim_embeddings, axis=1) *\n",
    "    np.linalg.norm(back_embeddings,  axis=1)\n",
    ")\n",
    "df[\"sem_sim\"] = numerators / denom\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4️⃣  Aggregate stats\n",
    "# ------------------------------------------------------------------\n",
    "# filter out rows where language is English\n",
    "df_noneng = df.query(\"language != 'English'\").reset_index(drop=True)\n",
    "\n",
    "overall_sim_noneng = df_noneng[\"sem_sim\"].mean()\n",
    "by_lang_noneng     = (df_noneng.groupby(\"language\")[\"sem_sim\"]\n",
    "                 .mean()\n",
    "                 .sort_values(ascending=False))\n",
    "\n",
    "print(f\"\\n=== Overall semantic similarity (cosine) === {overall_sim_noneng:.3f}\\n\")\n",
    "\n",
    "# overall standard deviation\n",
    "overall_std = df_noneng[\"sem_sim\"].std()\n",
    "print(f\"\\n=== Overall standard deviation === {overall_std:.3f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
