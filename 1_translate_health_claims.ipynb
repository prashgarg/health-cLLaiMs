{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. UK Health Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Define the path and sheet name\n",
    "file_path = \"data/NHC/great-britain-nutrition-and-health-claims-spreadsheet-26-March-2024.ods\"\n",
    "sheet_name = \"Health_claims\"\n",
    "\n",
    "# Read the ODS file using the 'odf' engine\n",
    "df = pd.read_excel(file_path, engine=\"odf\", sheet_name=sheet_name)\n",
    "\n",
    "# Clean up column names (strip leading/trailing whitespace)\n",
    "df.columns = df.columns.str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to concatenate multiple 'Scientific Opinion Reference' values with \"__\"\n",
    "def concat_refs(series):\n",
    "    # Drop NaNs, convert to string, take unique values, and join with \"__\"\n",
    "    return \"__\".join(series.dropna().astype(str).unique())\n",
    "\n",
    "# Function to take the first non-null value in a series\n",
    "def first_non_null(series):\n",
    "    non_null = series.dropna()\n",
    "    return non_null.iloc[0] if not non_null.empty else None\n",
    "\n",
    "# List of columns to aggregate. We want to group by 'Claim' and\n",
    "# for the 'Scientific Opinion Reference' column, concatenate values.\n",
    "# For all other columns, we'll take the first non-null value.\n",
    "agg_dict = {\n",
    "    \"Claim type\": first_non_null,\n",
    "    \"Nutrient substance, food or food category\": first_non_null,\n",
    "    \"Claim\": \"first\",  # Grouping column, so just take one copy\n",
    "    \"Conditions of use of the claim / Restrictions of use / Reasons for non-authorisation\": first_non_null,\n",
    "    \"Health relationship\": first_non_null,\n",
    "    \"Scientific Opinion Reference\": concat_refs,\n",
    "    \"Regulation (Note: All claims after the 1st January 2021 were GB approved. Claims prior to the 1st of January 2021 were approved by the EU, and retained by GB.)\": first_non_null,\n",
    "    \"Status (Note: Asterisked claims (*) were authorised on the basis of proprietary data and are also listed in the Annex to GB Nutrition and Health Claims Register)\": first_non_null,\n",
    "    \"Entry Id\": first_non_null  # Adjust column name if needed (removed extra spaces)\n",
    "}\n",
    "\n",
    "# Before grouping, ensure the column names match exactly.\n",
    "# If the \"Entry Id\" column in your file has extra spaces, you might need to adjust accordingly.\n",
    "\n",
    "# Group the dataframe by 'Claim' and aggregate the values\n",
    "df_grouped = df.groupby(\"Claim\", as_index=False).agg(agg_dict)\n",
    "\n",
    "# Create a unique claim_id column (starting at 1)\n",
    "df_grouped.insert(0, \"claim_id\", range(1, len(df_grouped) + 1))\n",
    "\n",
    "# (Optional) Save the processed dataframe to a new CSV file for verification\n",
    "df_grouped.to_csv(\"int_data/UK_processed_health_claims.csv\", index=False)\n",
    "\n",
    "# Print a sample of the dataframe to verify results\n",
    "print(df_grouped.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read processed_health_claims\n",
    "df_grouped = pd.read_csv(\"int_data/UK_processed_health_claims.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"KEY\" # Enter your key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# Step 0: Define the translation prompt components\n",
    "# =============================================================================\n",
    "\n",
    "# Define the response format JSON schema for claim translation with three translation keys\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"claim_translation_v1\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"translation_claim\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": (\n",
    "                        \"The translated health claim text in the target language. \"\n",
    "                        \"If a specialized medical term is not available in the target language, retain the original English term.\"\n",
    "                    )\n",
    "                },\n",
    "                \"translation_nutrient_substance\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": (\n",
    "                        \"The translated text for the nutrient substance, food or food category in the target language.\"\n",
    "                    )\n",
    "                },\n",
    "                \"translation_health_relationship\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": (\n",
    "                        \"The translated text for the health relationship in the target language.\"\n",
    "                    )\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"translation_claim\", \"translation_nutrient_substance\", \"translation_health_relationship\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the system instruction for the LLM as a translation expert\n",
    "system_instruction = (\n",
    "    \"You are an expert translator specializing in medical claims. Your task is to accurately translate the provided text from English into the target language specified in the user prompt. \"\n",
    "    \"This text includes three parts: the health claim, the nutrient substance (or food/food category), and the health relationship. \"\n",
    "    \"Please provide your translations using the keys 'translation_claim', 'translation_nutrient_substance', and 'translation_health_relationship'. \"\n",
    "    \"Be precise and maintain the original meaning. If a specialized medical term does not exist in the target language, keep the original English term. \"\n",
    "    \"Your response must strictly adhere to the JSON schema provided in the response format.\"\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# Function to create a JSONL entry for a given health claim translation prompt\n",
    "# =============================================================================\n",
    "def create_jsonl_entry_from_claim(claim, claim_id, nutrient, health_relationship, target_language):\n",
    "    \"\"\"\n",
    "    Create a JSONL entry to query an LLM for health claim translation.\n",
    "\n",
    "    Parameters:\n",
    "        claim (str): The health claim text in English.\n",
    "        claim_id (int): Unique identifier for the claim.\n",
    "        nutrient (str): The nutrient substance, food or food category.\n",
    "        health_relationship (str): The health relationship.\n",
    "        target_language (str): The target language to translate the text into.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary representing the JSON payload for the LLM prompt.\n",
    "    \"\"\"\n",
    "    custom_id = f\"claim_{claim_id}_{target_language}\"\n",
    "    user_prompt = (\n",
    "        f\"Translate the following text into '{target_language}':\\n\"\n",
    "        f\"Claim: {claim}\\n\"\n",
    "        f\"Nutrient substance, food or food category: {nutrient}\\n\"\n",
    "        f\"Health relationship: {health_relationship}\"\n",
    "    )\n",
    "    \n",
    "    entry = {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_instruction\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 10000,\n",
    "            \"response_format\": response_format\n",
    "        }\n",
    "    }\n",
    "    return entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top internet website languages are from https://en.wikipedia.org/wiki/Languages_used_on_the_Internet?utm_source=chatgpt.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 1: Create the JSONL file for batch processing from health claims DataFrame\n",
    "# =============================================================================\n",
    "\n",
    "# Define the target languages (ignoring the provided percentages)\n",
    "target_languages = [\"English\", \"Spanish\", \"Russian\", \"German\", \"French\", \n",
    "                    \"Japanese\", \"Portuguese\", \"Turkish\", \"Italian\", \"Persian\", \n",
    "                    \"Dutch\", \"Polish\", \"Chinese\", \"Vietnamese\", \"Indonesian\", \n",
    "                    \"Czech\", \"Korean\", \"Ukrainian\", \"Arabic\", \"Greek\", \n",
    "                    \"Hindi\" \n",
    "                    ]\n",
    "\n",
    "jsonl_filename = \"batch_input_hc_multilingual.jsonl\"\n",
    "with open(jsonl_filename, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for _, row in df_grouped.iterrows():\n",
    "        for lang in target_languages:\n",
    "            entry = create_jsonl_entry_from_claim(\n",
    "                row['Claim'],\n",
    "                row['claim_id'],\n",
    "                row['Nutrient substance, food or food category'],\n",
    "                row['Health relationship'],\n",
    "                lang\n",
    "            )\n",
    "            jsonl_file.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "print(\"JSONL file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Step 2: Split the JSONL file into smaller chunks (<100mb each)\n",
    "# =============================================================================\n",
    "\n",
    "# Create input_batches_hc_multilingual directory if it doesn't exist\n",
    "if not os.path.exists(\"input_batches_hc_multilingual\"):\n",
    "    os.makedirs(\"input_batches_hc_multilingual\")\n",
    "\n",
    "def split_jsonl_file(file_path, num_batches):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    total_lines = len(lines)\n",
    "    lines_per_batch = total_lines // num_batches\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        batch_lines = lines[i * lines_per_batch : (i + 1) * lines_per_batch]\n",
    "        batch_file_path = f\"input_batches_hc_multilingual/batch_input_hc_multilingual_part{i + 1}.jsonl\"\n",
    "        with open(batch_file_path, 'w', encoding='utf-8') as batch_file:\n",
    "            batch_file.writelines(batch_lines)\n",
    "    \n",
    "    # If there are leftover lines, add them to the last batch\n",
    "    if total_lines % num_batches != 0:\n",
    "        with open(f\"input_batches_hc_multilingual/batch_input_hc_multilingual_part{num_batches}.jsonl\", 'a', encoding='utf-8') as batch_file:\n",
    "            batch_file.writelines(lines[num_batches * lines_per_batch :])\n",
    "    \n",
    "    print(\"Batches created successfully.\")\n",
    "\n",
    "# Adjust the number of batches to 5\n",
    "split_jsonl_file(jsonl_filename, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Step 3: Upload the JSONL files and create batch jobs using the OpenAI client\n",
    "# =============================================================================\n",
    "\n",
    "# Import the OpenAI client (assuming it's defined similarly to your existing code)\n",
    "from openai import OpenAI\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"  # Set your API key here\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Function to create a sample JSONL file with 1 random line\n",
    "def create_sample_jsonl(file_path, sample_size=1):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Randomly select sample_size lines from the full file\n",
    "    sample_lines = random.sample(lines, sample_size)\n",
    "    \n",
    "    # Create a smaller JSONL file with just the sample lines\n",
    "    sample_file_path = \"sample_input_hc_multilingual.jsonl\"\n",
    "    with open(sample_file_path, 'w', encoding='utf-8') as sample_file:\n",
    "        sample_file.writelines(sample_lines)\n",
    "    \n",
    "    print(f\"Sample of {sample_size} lines created successfully in {sample_file_path}.\")\n",
    "    return sample_file_path\n",
    "\n",
    "# Function to upload the sample file and create a batch job\n",
    "def upload_and_create_sample_batch(sample_file_path):\n",
    "    # Upload the sample JSONL file\n",
    "    sample_input_file = client.files.create(\n",
    "        file=open(sample_file_path, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "    \n",
    "    sample_input_file_id = sample_input_file.id\n",
    "    \n",
    "    # Create a batch job with the sample file\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=sample_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": \"Pilot health claim translation job with sample line(s)\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Sample batch job created successfully with Batch ID: {batch.id}\")\n",
    "    return batch.id\n",
    "\n",
    "# Run the sample creation process\n",
    "sample_file = create_sample_jsonl(\"batch_input_hc_multilingual.jsonl\", 1)\n",
    "sample_batch_id = upload_and_create_sample_batch(sample_file)\n",
    "\n",
    "# Function to upload and create batches for all parts\n",
    "def upload_and_create_batches(num_batches):\n",
    "    batch_ids = []\n",
    "    for i in range(1, num_batches + 1):\n",
    "        batch_file_path = f\"input_batches_hc_multilingual/batch_input_hc_multilingual_part{i}.jsonl\"\n",
    "        \n",
    "        # Upload the JSONL file for batch processing\n",
    "        batch_input_file = client.files.create(\n",
    "            file=open(batch_file_path, \"rb\"),\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "        \n",
    "        batch_input_file_id = batch_input_file.id\n",
    "        \n",
    "        # Create the batch job\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=batch_input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\n",
    "                \"description\": f\"Health claim translation job part {i}\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        batch_ids.append(batch.id)\n",
    "        print(f\"Batch {i} job created successfully with Batch ID: {batch.id}\")\n",
    "        \n",
    "        # Pause between uploads to avoid hitting rate limits\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return batch_ids\n",
    "\n",
    "# Run the function to create all batch jobs (using 5 batches)\n",
    "num_batches = 2\n",
    "batch_ids = upload_and_create_batches(num_batches)\n",
    "\n",
    "# (Optional) Save the batch IDs for later retrieval\n",
    "with open(\"batch_ids_hc_multilingual.pkl\", \"wb\") as file:\n",
    "    pickle.dump(batch_ids, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Step 4: Check the status of each batch and retrieve the results\n",
    "# =============================================================================\n",
    "\n",
    "# Create output_batches_hc_multilingual directory if it doesn't exist\n",
    "if not os.path.exists(\"output_batches_hc_multilingual\"):\n",
    "    os.makedirs(\"output_batches_hc_multilingual\")\n",
    "\n",
    "def check_batch_status(batch_ids):\n",
    "    for batch_id in batch_ids:\n",
    "        batch_status = client.batches.retrieve(batch_id)\n",
    "        print(f\"Status for Batch ID {batch_id}: {batch_status.status}\")\n",
    "        \n",
    "        if batch_status.status == 'completed':\n",
    "            # Retrieve the output file\n",
    "            output_file_id = batch_status.output_file_id\n",
    "            file_response = client.files.content(output_file_id)\n",
    "            output_file_path = f\"output_batches_hc_multilingual/batch_output_hc_multilingual_{batch_id}.jsonl\"\n",
    "            with open(output_file_path, \"w\", encoding='utf-8') as output_file:\n",
    "                output_file.write(file_response.text)\n",
    "            print(f\"Batch output saved to {output_file_path}\")\n",
    "        else:\n",
    "            print(f\"Batch {batch_id} not completed yet. Please check again later.\")\n",
    "\n",
    "# Example usage: Check the status of the batches\n",
    "check_batch_status(batch_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Step 5: Combine the results from all batches into a single DataFrame and save as CSV\n",
    "# =============================================================================\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load a JSONL file into a list of dictionaries.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [json.loads(line) for line in file.readlines()]\n",
    "\n",
    "def normalize_claim_responses(data):\n",
    "    \"\"\"\n",
    "    Process and normalize the response format for health claim translation.\n",
    "    Extract the three translation fields from the LLM's response.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for entry in data:\n",
    "        custom_id = entry.get('custom_id', 'unknown_id')\n",
    "        if (\n",
    "            'response' in entry and\n",
    "            'body' in entry['response'] and\n",
    "            'choices' in entry['response']['body']\n",
    "        ):\n",
    "            for choice in entry['response']['body']['choices']:\n",
    "                message_content = choice['message']['content']\n",
    "                try:\n",
    "                    content_json = json.loads(message_content)\n",
    "                    record = {\n",
    "                        \"custom_id\": custom_id,\n",
    "                        \"translation_claim\": content_json.get(\"translation_claim\", \"NA\"),\n",
    "                        \"translation_nutrient_substance\": content_json.get(\"translation_nutrient_substance\", \"NA\"),\n",
    "                        \"translation_health_relationship\": content_json.get(\"translation_health_relationship\", \"NA\")\n",
    "                    }\n",
    "                    records.append(record)\n",
    "                except (json.JSONDecodeError, TypeError) as e:\n",
    "                    logging.error(f\"Failed to parse content for custom_id {custom_id}. Error: {e}\")\n",
    "                    logging.error(f\"Content: {message_content}\")\n",
    "        else:\n",
    "            logging.error(f\"No valid response found in entry with custom_id {custom_id}\")\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "def load_and_parse_batches(batch_files_dir):\n",
    "    \"\"\"\n",
    "    Load all batch files from the specified directory,\n",
    "    process them, and combine the results into a single DataFrame.\n",
    "    \"\"\"\n",
    "    batch_files = [\n",
    "        os.path.join(batch_files_dir, f)\n",
    "        for f in os.listdir(batch_files_dir)\n",
    "        if f.endswith('.jsonl')\n",
    "    ]\n",
    "    all_records = []\n",
    "    \n",
    "    # Configure logging for errors\n",
    "    logging.basicConfig(\n",
    "        filename='parsing_errors_hc_multilingual.log',\n",
    "        level=logging.ERROR,\n",
    "        format='%(asctime)s:%(levelname)s:%(message)s'\n",
    "    )\n",
    "    \n",
    "    for batch_file in tqdm(batch_files, desc=\"Processing batch files\"):\n",
    "        print(f\"Processing batch file: {batch_file}\")\n",
    "        batch_data = load_jsonl(batch_file)\n",
    "        df_batch = normalize_claim_responses(batch_data)\n",
    "        if not df_batch.empty:\n",
    "            all_records.append(df_batch)\n",
    "    \n",
    "    combined_df = pd.concat(all_records, ignore_index=True) if all_records else pd.DataFrame()\n",
    "    return combined_df\n",
    "\n",
    "# Update the directory as needed where your batch JSONL files are located\n",
    "batch_files_dir = 'output_batches_hc_multilingual/'\n",
    "\n",
    "# Load and parse the batches into a single DataFrame\n",
    "df_translations = load_and_parse_batches(batch_files_dir)\n",
    "\n",
    "# Replace any NaNs with 'NA' for consistency\n",
    "df_translations.fillna('NA', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'custom_id' column into three parts: prefix, claim_id, and language\n",
    "df_translations[['prefix', 'claim_id', 'language']] = df_translations['custom_id'].str.split('_', expand=True)\n",
    "\n",
    "# Drop the prefix column (which contains the literal 'claim')\n",
    "df_translations.drop(columns='prefix', inplace=True)\n",
    "\n",
    "# Optionally, convert claim_id to an integer (if it contains numeric values)\n",
    "df_translations['claim_id'] = df_translations['claim_id'].astype(int)\n",
    "\n",
    "# Perform a left join on the claim_id column\n",
    "df_translations = df_translations.merge(df_grouped[['claim_id', 'Claim']], on='claim_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "translations_output_csv = 'int_data/UK_extracted_hc_multilingual.csv'\n",
    "df_translations.to_csv(translations_output_csv, index=False)\n",
    "\n",
    "print(f\"Claim translations have been successfully flattened and saved to {translations_output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EU Health Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Define the path and sheet name\n",
    "file_path = \"data/EUHC/EU_Register_on_nutrition_and_health_claims.xlsx\"\n",
    "sheet_name = \"Worksheet\"\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean up column names (strip leading/trailing whitespace)\n",
    "df.columns = df.iloc[0].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to concatenate multiple 'Scientific Opinion Reference' values with \"__\"\n",
    "def concat_refs(series):\n",
    "    # Drop NaNs, convert to string, take unique values, and join with \"__\"\n",
    "    return \"__\".join(series.dropna().astype(str).unique())\n",
    "\n",
    "# Function to take the first non-null value in a series\n",
    "def first_non_null(series):\n",
    "    non_null = series.dropna()\n",
    "    return non_null.iloc[0] if not non_null.empty else None\n",
    "\n",
    "# List of columns to aggregate. We want to group by 'Claim' and\n",
    "# for the 'Scientific Opinion Reference' column, concatenate values.\n",
    "# For all other columns, we'll take the first non-null value.\n",
    "agg_dict = {\n",
    "    \"Claim type\": first_non_null,\n",
    "    \"Nutrient substance, food or food category\": first_non_null,\n",
    "    \"Claim\": \"first\",  # Grouping column, so just take one copy\n",
    "    \"Conditions of use of the claim / Restrictions of use / Reasons for non-authorisation\": first_non_null,\n",
    "    \"Health relationship\": first_non_null,\n",
    "    \"EFSA opinion reference\": concat_refs,\n",
    "    \"Commission Regulation\": first_non_null,\n",
    "    \"Status\": first_non_null,\n",
    "    \"Entry Id\": first_non_null  # Adjust column name if needed (removed extra spaces)\n",
    "}\n",
    "\n",
    "# Before grouping, ensure the column names match exactly.\n",
    "# If the \"Entry Id\" column in your file has extra spaces, you might need to adjust accordingly.\n",
    "\n",
    "# Group the dataframe by 'Claim' and aggregate the values\n",
    "df_grouped = df.groupby(\"Claim\", as_index=False).agg(agg_dict)\n",
    "\n",
    "# Create a unique claim_id column (starting at 1)\n",
    "df_grouped.insert(0, \"claim_id\", range(1, len(df_grouped) + 1))\n",
    "\n",
    "# (Optional) Save the processed dataframe to a new CSV file for verification\n",
    "df_grouped.to_csv(\"int_data/EU_processed_health_claims.csv\", index=False)\n",
    "\n",
    "# Print a sample of the dataframe to verify results\n",
    "print(df_grouped.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read processed_health_claims\n",
    "df_grouped = pd.read_csv(\"int_data/EU_processed_health_claims.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"Key\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# Step 0: Define the translation prompt components\n",
    "# =============================================================================\n",
    "\n",
    "# Define the response format JSON schema for claim translation with three translation keys\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"claim_translation_v1\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"translation_claim\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": (\n",
    "                        \"The translated health claim text in the target language. \"\n",
    "                        \"If a specialized medical term is not available in the target language, retain the original English term.\"\n",
    "                    )\n",
    "                },\n",
    "                \"translation_nutrient_substance\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": (\n",
    "                        \"The translated text for the nutrient substance, food or food category in the target language.\"\n",
    "                    )\n",
    "                },\n",
    "                \"translation_health_relationship\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": (\n",
    "                        \"The translated text for the health relationship in the target language.\"\n",
    "                    )\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"translation_claim\", \"translation_nutrient_substance\", \"translation_health_relationship\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the system instruction for the LLM as a translation expert\n",
    "system_instruction = (\n",
    "    \"You are an expert translator specializing in medical claims. Your task is to accurately translate the provided text from English into the target language specified in the user prompt. \"\n",
    "    \"This text includes three parts: the health claim, the nutrient substance (or food/food category), and the health relationship. \"\n",
    "    \"Please provide your translations using the keys 'translation_claim', 'translation_nutrient_substance', and 'translation_health_relationship'. \"\n",
    "    \"Be precise and maintain the original meaning. If a specialized medical term does not exist in the target language, keep the original English term. \"\n",
    "    \"Your response must strictly adhere to the JSON schema provided in the response format.\"\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# Function to create a JSONL entry for a given health claim translation prompt\n",
    "# =============================================================================\n",
    "def create_jsonl_entry_from_claim(claim, claim_id, nutrient, health_relationship, target_language):\n",
    "    \"\"\"\n",
    "    Create a JSONL entry to query an LLM for health claim translation.\n",
    "\n",
    "    Parameters:\n",
    "        claim (str): The health claim text in English.\n",
    "        claim_id (int): Unique identifier for the claim.\n",
    "        nutrient (str): The nutrient substance, food or food category.\n",
    "        health_relationship (str): The health relationship.\n",
    "        target_language (str): The target language to translate the text into.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary representing the JSON payload for the LLM prompt.\n",
    "    \"\"\"\n",
    "    custom_id = f\"claim_{claim_id}_{target_language}\"\n",
    "    user_prompt = (\n",
    "        f\"Translate the following text into '{target_language}':\\n\"\n",
    "        f\"Claim: {claim}\\n\"\n",
    "        f\"Nutrient substance, food or food category: {nutrient}\\n\"\n",
    "        f\"Health relationship: {health_relationship}\"\n",
    "    )\n",
    "    \n",
    "    entry = {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_instruction\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 10000,\n",
    "            \"response_format\": response_format\n",
    "        }\n",
    "    }\n",
    "    return entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top internet website languages are from https://en.wikipedia.org/wiki/Languages_used_on_the_Internet?utm_source=chatgpt.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 1: Create the JSONL file for batch processing from health claims DataFrame\n",
    "# =============================================================================\n",
    "\n",
    "# Define the target languages (ignoring the provided percentages)\n",
    "target_languages = [\"English\", \"Spanish\", \"Russian\", \"German\", \"French\", \n",
    "                    \"Japanese\", \"Portuguese\", \"Turkish\", \"Italian\", \"Persian\", \n",
    "                    \"Dutch\", \"Polish\", \"Chinese\", \"Vietnamese\", \"Indonesian\", \n",
    "                    \"Czech\", \"Korean\", \"Ukrainian\", \"Arabic\", \"Greek\", \n",
    "                    \"Hindi\" \n",
    "                    ]\n",
    "\n",
    "jsonl_filename = \"int_data/EUHC/batch_input_euhc_multilingual.jsonl\"\n",
    "with open(jsonl_filename, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for _, row in df_grouped.iterrows():\n",
    "        for lang in target_languages:\n",
    "            entry = create_jsonl_entry_from_claim(\n",
    "                row['Claim'],\n",
    "                row['claim_id'],\n",
    "                row['Nutrient substance, food or food category'],\n",
    "                row['Health relationship'],\n",
    "                lang\n",
    "            )\n",
    "            jsonl_file.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "print(\"JSONL file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Step 2: Split the JSONL file into smaller chunks (<100mb each)\n",
    "# =============================================================================\n",
    "\n",
    "# Create input_batches_hc_multilingual directory if it doesn't exist\n",
    "if not os.path.exists(\"int_data/EUHC/input_batches_hc_multilingual\"):\n",
    "    os.makedirs(\"int_data/EUHC/input_batches_hc_multilingual\")\n",
    "\n",
    "def split_jsonl_file(file_path, num_batches):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    total_lines = len(lines)\n",
    "    lines_per_batch = total_lines // num_batches\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        batch_lines = lines[i * lines_per_batch : (i + 1) * lines_per_batch]\n",
    "        batch_file_path = f\"int_data/EUHC/input_batches_hc_multilingual/batch_input_hc_multilingual_part{i + 1}.jsonl\"\n",
    "        with open(batch_file_path, 'w', encoding='utf-8') as batch_file:\n",
    "            batch_file.writelines(batch_lines)\n",
    "    \n",
    "    # If there are leftover lines, add them to the last batch\n",
    "    if total_lines % num_batches != 0:\n",
    "        with open(f\"int_data/EUHC/input_batches_hc_multilingual/batch_input_hc_multilingual_part{num_batches}.jsonl\", 'a', encoding='utf-8') as batch_file:\n",
    "            batch_file.writelines(lines[num_batches * lines_per_batch :])\n",
    "    \n",
    "    print(\"Batches created successfully.\")\n",
    "\n",
    "# Adjust the number of batches to 5\n",
    "split_jsonl_file(jsonl_filename, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Step 3: Upload the JSONL files and create batch jobs using the OpenAI client\n",
    "# =============================================================================\n",
    "\n",
    "# Import the OpenAI client (assuming it's defined similarly to your existing code)\n",
    "from openai import OpenAI\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"  # Set your API key here\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Function to create a sample JSONL file with 1 random line\n",
    "def create_sample_jsonl(file_path, sample_size=1):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Randomly select sample_size lines from the full file\n",
    "    sample_lines = random.sample(lines, sample_size)\n",
    "    \n",
    "    # Create a smaller JSONL file with just the sample lines\n",
    "    sample_file_path = \"int_data/EUHC/sample_input_hc_multilingual.jsonl\"\n",
    "    with open(sample_file_path, 'w', encoding='utf-8') as sample_file:\n",
    "        sample_file.writelines(sample_lines)\n",
    "    \n",
    "    print(f\"Sample of {sample_size} lines created successfully in {sample_file_path}.\")\n",
    "    return sample_file_path\n",
    "\n",
    "# Function to upload the sample file and create a batch job\n",
    "def upload_and_create_sample_batch(sample_file_path):\n",
    "    # Upload the sample JSONL file\n",
    "    sample_input_file = client.files.create(\n",
    "        file=open(sample_file_path, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "    \n",
    "    sample_input_file_id = sample_input_file.id\n",
    "    \n",
    "    # Create a batch job with the sample file\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=sample_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": \"Pilot health claim translation job with sample line(s)\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Sample batch job created successfully with Batch ID: {batch.id}\")\n",
    "    return batch.id\n",
    "\n",
    "# Run the sample creation process\n",
    "sample_file = create_sample_jsonl(\"int_data/EUHC/batch_input_euhc_multilingual.jsonl\", 1)\n",
    "sample_batch_id = upload_and_create_sample_batch(sample_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to upload and create batches for all parts\n",
    "def upload_and_create_batches(num_batches):\n",
    "    batch_ids = []\n",
    "    for i in range(1, num_batches + 1):\n",
    "        batch_file_path = f\"int_data/EUHC/input_batches_hc_multilingual/batch_input_hc_multilingual_part{i}.jsonl\"\n",
    "        \n",
    "        # Upload the JSONL file for batch processing\n",
    "        batch_input_file = client.files.create(\n",
    "            file=open(batch_file_path, \"rb\"),\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "        \n",
    "        batch_input_file_id = batch_input_file.id\n",
    "        \n",
    "        # Create the batch job\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=batch_input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\n",
    "                \"description\": f\"Health claim translation job part {i}\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        batch_ids.append(batch.id)\n",
    "        print(f\"Batch {i} job created successfully with Batch ID: {batch.id}\")\n",
    "        \n",
    "        # Pause between uploads to avoid hitting rate limits\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return batch_ids\n",
    "\n",
    "# Run the function to create all batch jobs (using 5 batches)\n",
    "num_batches = 2\n",
    "batch_ids = upload_and_create_batches(num_batches)\n",
    "\n",
    "# (Optional) Save the batch IDs for later retrieval\n",
    "with open(\"int_data/EUHC/batch_ids_hc_multilingual.pkl\", \"wb\") as file:\n",
    "    pickle.dump(batch_ids, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Step 4: Check the status of each batch and retrieve the results\n",
    "# =============================================================================\n",
    "\n",
    "# Create output_batches_hc_multilingual directory if it doesn't exist\n",
    "if not os.path.exists(\"int_data/EUHC/output_batches_hc_multilingual\"):\n",
    "    os.makedirs(\"int_data/EUHC/output_batches_hc_multilingual\")\n",
    "\n",
    "def check_batch_status(batch_ids):\n",
    "    for batch_id in batch_ids:\n",
    "        batch_status = client.batches.retrieve(batch_id)\n",
    "        print(f\"Status for Batch ID {batch_id}: {batch_status.status}\")\n",
    "        \n",
    "        if batch_status.status == 'completed':\n",
    "            # Retrieve the output file\n",
    "            output_file_id = batch_status.output_file_id\n",
    "            file_response = client.files.content(output_file_id)\n",
    "            output_file_path = f\"int_data/EUHC/output_batches_hc_multilingual/batch_output_hc_multilingual_{batch_id}.jsonl\"\n",
    "            with open(output_file_path, \"w\", encoding='utf-8') as output_file:\n",
    "                output_file.write(file_response.text)\n",
    "            print(f\"Batch output saved to {output_file_path}\")\n",
    "        else:\n",
    "            print(f\"Batch {batch_id} not completed yet. Please check again later.\")\n",
    "\n",
    "# Example usage: Check the status of the batches\n",
    "check_batch_status(batch_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Step 5: Combine the results from all batches into a single DataFrame and save as CSV\n",
    "# =============================================================================\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load a JSONL file into a list of dictionaries.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [json.loads(line) for line in file.readlines()]\n",
    "\n",
    "def normalize_claim_responses(data):\n",
    "    \"\"\"\n",
    "    Process and normalize the response format for health claim translation.\n",
    "    Extract the three translation fields from the LLM's response.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for entry in data:\n",
    "        custom_id = entry.get('custom_id', 'unknown_id')\n",
    "        if (\n",
    "            'response' in entry and\n",
    "            'body' in entry['response'] and\n",
    "            'choices' in entry['response']['body']\n",
    "        ):\n",
    "            for choice in entry['response']['body']['choices']:\n",
    "                message_content = choice['message']['content']\n",
    "                try:\n",
    "                    content_json = json.loads(message_content)\n",
    "                    record = {\n",
    "                        \"custom_id\": custom_id,\n",
    "                        \"translation_claim\": content_json.get(\"translation_claim\", \"NA\"),\n",
    "                        \"translation_nutrient_substance\": content_json.get(\"translation_nutrient_substance\", \"NA\"),\n",
    "                        \"translation_health_relationship\": content_json.get(\"translation_health_relationship\", \"NA\")\n",
    "                    }\n",
    "                    records.append(record)\n",
    "                except (json.JSONDecodeError, TypeError) as e:\n",
    "                    logging.error(f\"Failed to parse content for custom_id {custom_id}. Error: {e}\")\n",
    "                    logging.error(f\"Content: {message_content}\")\n",
    "        else:\n",
    "            logging.error(f\"No valid response found in entry with custom_id {custom_id}\")\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "def load_and_parse_batches(batch_files_dir):\n",
    "    \"\"\"\n",
    "    Load all batch files from the specified directory,\n",
    "    process them, and combine the results into a single DataFrame.\n",
    "    \"\"\"\n",
    "    batch_files = [\n",
    "        os.path.join(batch_files_dir, f)\n",
    "        for f in os.listdir(batch_files_dir)\n",
    "        if f.endswith('.jsonl')\n",
    "    ]\n",
    "    all_records = []\n",
    "    \n",
    "    # Configure logging for errors\n",
    "    logging.basicConfig(\n",
    "        filename='int_data/EUHC/parsing_errors_hc_multilingual.log',\n",
    "        level=logging.ERROR,\n",
    "        format='%(asctime)s:%(levelname)s:%(message)s'\n",
    "    )\n",
    "    \n",
    "    for batch_file in tqdm(batch_files, desc=\"Processing batch files\"):\n",
    "        print(f\"Processing batch file: {batch_file}\")\n",
    "        batch_data = load_jsonl(batch_file)\n",
    "        df_batch = normalize_claim_responses(batch_data)\n",
    "        if not df_batch.empty:\n",
    "            all_records.append(df_batch)\n",
    "    \n",
    "    combined_df = pd.concat(all_records, ignore_index=True) if all_records else pd.DataFrame()\n",
    "    return combined_df\n",
    "\n",
    "# Update the directory as needed where your batch JSONL files are located\n",
    "batch_files_dir = 'int_data/EUHC/output_batches_hc_multilingual/'\n",
    "\n",
    "# Load and parse the batches into a single DataFrame\n",
    "df_translations = load_and_parse_batches(batch_files_dir)\n",
    "\n",
    "# Replace any NaNs with 'NA' for consistency\n",
    "df_translations.fillna('NA', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'custom_id' column into three parts: prefix, claim_id, and language\n",
    "df_translations[['prefix', 'claim_id', 'language']] = df_translations['custom_id'].str.split('_', expand=True)\n",
    "\n",
    "# Drop the prefix column (which contains the literal 'claim')\n",
    "df_translations.drop(columns='prefix', inplace=True)\n",
    "\n",
    "# Optionally, convert claim_id to an integer (if it contains numeric values)\n",
    "df_translations['claim_id'] = df_translations['claim_id'].astype(int)\n",
    "\n",
    "# Perform a left join on the claim_id column\n",
    "df_translations = df_translations.merge(df_grouped[['claim_id', 'Claim']], on='claim_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "translations_output_csv = 'int_data/EU_extracted_hc_multilingual.csv'\n",
    "df_translations.to_csv(translations_output_csv, index=False)\n",
    "\n",
    "print(f\"Claim translations have been successfully flattened and saved to {translations_output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
