---
title: "5_gephi_bec"
author: "Prashant Garg"
date: "2024-06-06"
output: html_document
---

#init
```{r}
library(data.table)
library(magrittr)
library(jsonlite)
library(tidyverse)
library(ggrepel)
```

# data
## 1. linguistic distance data

Full levenshteinLanguageDistances.csv is 800mb and easily available from ASJP's official website
Here, we have provided the intermediary data that goes through the commented out selection of steps.
The steps ensure we have dyadic English-other language distance pairs.
```{r}
# dfL <- read_csv("data/ASJP/levenshteinLanguageDistances.csv")
# dfL %<>% filter(language1=="ENGLISH"|language2=="ENGLISH")
# 
# # Define your 21 targets
# targets <- c(
#   "English","Spanish","Russian","STANDARD_GERMAN","French",
#   "Japanese","Portuguese","Turkish","Italian","Persian",
#   "Dutch","Polish","MIDDLE_CHINESE","Vietnamese","Indonesian",
#   "Czech","Korean","Ukrainian","STANDARD_ARABIC","Greek","Hindi"
# )
# targets<-toupper(targets)
# 
# # 3. Filter: English on one side, target language on the other
# dfL_filt <- dfL %>%
#   filter(
#     (language1 == "ENGLISH" & language2 %in% targets) |
#     (language2 == "ENGLISH" & language1 %in% targets)
#   )
# 
# # 4. Extract the non‑English side for each row
# obtained <- dfL_filt %>%
#   mutate(other = if_else(language1 == "ENGLISH", language2, language1)) %>%
#   pull(other) %>%
#   unique()
# 
# # 5. See which targets are missing
# missing <- setdiff(targets, obtained)
# 
# dfL_filt %<>% rename(distance=`L-distance`)
# 
# print(missing)

# write_csv(dfL,"data/ASJP/levenshteinLanguageDistances_processed.csv")
dfL <- read_csv("data/ASJP/levenshteinLanguageDistances_processed.csv")

```


## 2. UK HC
metadata
```{r}
df_UK = read_csv("int_data/UK_processed_health_claims.csv")

```

responses
```{r}
# Load required packages
library(jsonlite)

# 1. List all JSON files in the folder that start with "responses_"
json_files <- list.files(
  path = "int_data/responses_UKHC/",
  pattern = "^responses_.*\\.json$",
  full.names = TRUE
)

# 2. Read each JSON into a data frame
dfs_UK <- lapply(json_files, function(file) {
  # Assumes each JSON file is an array of records (list of objects)
  fromJSON(file, flatten = TRUE)
})

# 3. Combine all data frames into one
df_llm_UK <- bind_rows(dfs_UK)
rm(dfs_UK)
```


```{r}
setDT(df_llm_UK)
# 1. Split off the model and iteration (by “__”)
df_llm_UK[, c("prefix", "model", "iteration") := tstrsplit(custom_id, "__", fixed = TRUE)]

# 2. Split the prefix into the “claim” literal, claim_id, and language (by “_”)
df_llm_UK[, c("dummy", "claim_id", "language") := tstrsplit(prefix, "_", fixed = TRUE)]

# 3. Clean up helper columns and convert iteration to integer
df_llm_UK[, `:=`(
  prefix = NULL,
  dummy = NULL,
  iteration = as.integer(iteration)
)]
```

```{r}
old_status_name <- "Status (Note: Asterisked claims (*) were authorised on the basis of proprietary data and are also listed in the Annex to GB Nutrition and Health Claims Register)"
setnames(df_UK, old = old_status_name, new = "status")

setDT(df_UK)
# 2. Keep only claim_id and status, and make sure claim_id is character to match df_llm
df_UK <- df_UK[, .(
  claim_id = as.character(claim_id),
  status
)]

# 3. Left‑join onto df_llm by claim_id, adding `status`
#    This will add NA where no match is found.
df_llm_UK <- merge(
  x = df_llm_UK,
  y = df_UK,
  by = "claim_id",
  all.x = TRUE
)

df_llm_UK[,status:=ifelse(status=="Authorised for use in Great Britain*","Authorised for use in Great Britain", status)]
df_llm_UK[,status:=ifelse(status=="Authorised for use in Great Britain","Authorised", status)]
```


## 3. EU HC
metadata
```{r}
df_EU = read_csv("int_data/EU_processed_health_claims.csv")

```

responses
```{r}
# Load required packages
library(jsonlite)

# 1. List all JSON files in the folder that start with "responses_"
json_files_EU <- list.files(
  path = "int_data/responses_EUHC/",
  pattern = "^responses_.*\\.json$",
  full.names = TRUE
)

# 2. Read each JSON into a data frame
dfs_EU <- lapply(json_files_EU, function(file) {
  # Assumes each JSON file is an array of records (list of objects)
  fromJSON(file, flatten = TRUE)
})

# 3. Combine all data frames into one
df_llm_EU <- bind_rows(dfs_EU)
rm(dfs_EU)
```

```{r}
setDT(df_llm_EU)
# 1. Split off the model and iteration (by “__”)
df_llm_EU[, c("prefix", "model", "iteration") := tstrsplit(custom_id, "__", fixed = TRUE)]

# 2. Split the prefix into the “claim” literal, claim_id, and language (by “_”)
df_llm_EU[, c("dummy", "claim_id", "language") := tstrsplit(prefix, "_", fixed = TRUE)]

# 3. Clean up helper columns and convert iteration to integer
df_llm_EU[, `:=`(
  prefix = NULL,
  dummy = NULL,
  iteration = as.integer(iteration)
)]
```

```{r}
library(data.table)

# Ensure both are data.tables
setDT(df_llm_EU)
setDT(df_EU)

# 1. Rename the long Status column in `df` to just "status"
#    Replace the old name below with your exact column name.
old_status_name <- "Status"
setnames(df_EU, old = old_status_name, new = "status")

setDT(df_EU)
# 2. Keep only claim_id and status, and make sure claim_id is character to match df_llm
df_EU <- df_EU[, .(
  claim_id = as.character(claim_id),
  status
)]

# 3. Left‑join onto df_llm by claim_id, adding `status`
#    This will add NA where no match is found.
df_llm_EU <- merge(
  x = df_llm_EU,
  y = df_EU,
  by = "claim_id",
  all.x = TRUE
)

df_llm_EU[, .N, by = status]

```


## merge EU, uk

```{r}

df_llm_EU %<>% mutate(jurisfication="EU")
df_llm_EU %<>% mutate(custom_id=paste0("EU", custom_id))
glimpse(df_llm_EU)
df_llm_UK %<>% mutate(jurisfication="UK")
df_llm_UK %<>% mutate(custom_id=paste0("UK", custom_id))

df_llm<-bind_rows(df_llm_EU,df_llm_UK)
rm(df_llm_EU,df_llm_UK)
gc()
```


# preprocess

```{r}
df_llm[,claim_id:=paste0(jurisfication,claim_id)]

```

clean output
```{r}
df_llm %<>% rename(response_raw=response)


df_llm[, response := {
  # Helper: for a single string, extract all '0' or '1' characters and return the last as integer
  extract_last01 <- function(txt) {
    # find all matches of the characters '0' or '1'
    matches <- regmatches(txt, gregexpr("[01]", txt))[[1]]
    if (length(matches) > 0) {
      return(as.integer(tail(matches, 1)))
    } else {
      return(NA_integer_)
    }
  }
  # Vectorise over the column
  vapply(response_raw, extract_last01, integer(1))
}]

# Check the distribution
a<-df_llm[, .N, by = response][order(response)]

df_llm[,response:=ifelse(response>1,NA,response)]

```


true label
```{r}
df_llm[,label:=1]
df_llm$response <- as.numeric(df_llm$response)
df_llm$iteration <- as.numeric(df_llm$iteration)

df_llm[,.N,by=model]
```

clean: remove NAs
```{r}
df_llm[iteration<4,
  .(  
    total     = .N,  
    missing   = sum(is.na(response)),  
    non_missing = sum(!is.na(response))  
  ), by = model]
```

# evaluation


```{r}
setDT(df_llm)
df_modal <- df_llm[, .(
  modal_response = {
    tb <- table(response)
    as.integer(names(tb)[which.max(tb)])
  },
  label  = label[1],
  status = status[1]
), by = .(claim_id, language, model)]

# Evaluation by model, language, and status
evaluation_lang_status <- df_modal[!is.na(modal_response)][!is.na(status)][model!="phi3.5:latest"][, .(
  TruePos   = sum(modal_response == 1 & label == 1),
  FalseNeg  = sum(modal_response == 0 & label == 1),
  Accuracy  = sum(modal_response == 1 & label == 1) / .N
), by = .(model, language, status)]



# Filter out unwanted models & statuses
eval_lang_filt <- evaluation_lang_status %>%
  filter(
    status == "Authorised"
    )

# Mean accuracy per language (averaging over models & status)
accuracy_lang <- eval_lang_filt %>%
  group_by(language) %>%
  summarize(Accuracy = mean(Accuracy, na.rm = TRUE), .groups = "drop")

# Prepare Levenshtein distance table: English ↔ other
distances <- dfL_filt %>%
  select(language1, language2, distance) %>%
  mutate(
    other = if_else(language1 == "ENGLISH", language2, language1)
  ) %>%
  select(language = other, distance) %>%
  distinct() %>%
  mutate(
    language = recode(language,
      STANDARD_GERMAN = "German",
      MIDDLE_CHINESE  = "Chinese",
      STANDARD_ARABIC = "Arabic"
    ),
    language = str_to_title(tolower(language))
  )

# Normalize accuracy_lang language names and join distances
plot_df <- accuracy_lang %>%
  mutate(language = str_to_title(tolower(language))) %>%
  left_join(distances, by = "language")

#  Compute per‐model slopes and mean accuracies
#    a) accuracy per model & language
accuracy_model_lang <- eval_lang_filt %>%
  group_by(model, language) %>%
  summarize(Accuracy = mean(Accuracy, na.rm = TRUE), .groups = "drop") %>%
  mutate(language = str_to_title(tolower(language)))

#    b) join to distances
model_dist_df <- accuracy_model_lang %>%
  left_join(distances, by = "language")

model_stats <- model_dist_df %>%
  mutate(
    model_clean = model %>%
      str_remove_all("-\\d{4}-\\d{2}-\\d{2}") %>%
      str_remove_all(":latest") %>%
      str_replace_all("gemini-2\\.0-flash-lite", "gemini-2-flash")
  ) %>%
  group_by(model_clean) %>%
  summarize(
    mean_acc = mean(Accuracy, na.rm = TRUE),
    slope    = coef(lm(Accuracy ~ distance, data = cur_data()))[2],
    .groups = "drop"
  ) %>%
  # pad positive slopes so they line up under minus signs
  mutate(
    slope_str = ifelse(
      slope < 0,
      sprintf("<span style='color:black'>%.2f</span>", slope),
      sprintf("&nbsp;%.2f", slope)
    ),
    label = sprintf(
      "**%s**: mean=%.2f; slope=%s",
      model_clean, mean_acc, slope_str
    )
  ) %>%
  # **here** reorder by slope ascending (most negative first)
  arrange(slope)

# then collapse in that order:
label_block <- paste(model_stats$label, collapse = "<br>")


# precompute the data‐range
xmin <- min(plot_df$distance,   na.rm = TRUE)
ymin <- min(plot_df$Accuracy,   na.rm = TRUE)


library(ggtext)  # for rich text annotations
# Draw the plot with rich‐text annotation in bottom‐left
ggplot(plot_df, aes(x = distance, y = Accuracy)) +
  geom_point(size = 4) +
  geom_text_repel(aes(label = language), max.overlaps = 20, size = 6) +
  geom_richtext(
    data = tibble(x = xmin + 0.22, y = ymin+0.007),
    aes(x = x, y = y, label = label_block),
    fill       = NA,
    label.color= NA,
    hjust      = 1,
    vjust      = 0,
    size       = 6,
    family     = "mono"
  ) +
  labs(
    title = "",
    x     = "Levenshtein Distance to English",
    y     = "Mean Accuracy"
  ) +
  theme_classic(base_size = 20) +
  theme(
    legend.position   = "none",
    axis.text.x       = element_text(angle = 45, hjust = 1),
    plot.title        = element_blank()
  )


ggsave(plot=last_plot(), filename="figures/fig1.jpg",width = 12,height = 8,dpi=300)
ggsave(plot=last_plot(), filename="figures/fig1.pdf",width = 12,height = 8,dpi=300)
```
