---
title: "5_gephi_bec"
author: "Prashant Garg"
date: "2024-06-06"
output: html_document
---

#init
```{r}
library(data.table)
library(magrittr)
library(jsonlite)
library(tidyverse)
library(ggrepel)
```

# data

## 1. linguistic distance data

Full SYNTACTIC.csv is 436mb and easily available from https://github.com/antonisa/lang2vec
Here, we have provided the intermediary data that goes through the commented out selection of steps.
The steps ensure we have dyadic English-other language distance pairs.
```{r}
# --------------------------------------------------------------
# Construct English‑to‑target syntactic distances for Figure 1
# --------------------------------------------------------------

# # --------------------------------------------------------------
# # 0.  File locations  <<—–  EDIT if your folder structure differs
# # --------------------------------------------------------------
# uriel_folder <- "data/lang2vec"
# out_file     <- "data/lang2vec/processed_syntactic_English_vs_targets"
# 
# # --------------------------------------------------------------
# # 1.  Target languages (pretty names and ISO‑639‑3 codes)
# # --------------------------------------------------------------
# targets_iso <- c(
#   "eng","spa","rus","deu","fra","jpn","por","tur","ita","pes",
#   "nld","pol","ltc","vie","ind","ces","kor","ukr","arb","ell","hin"
# )
# 
# pretty_names <- c(
#   "English","Spanish","Russian","German","French",
#   "Japanese","Portuguese","Turkish","Italian","Persian",
#   "Dutch","Polish","Chinese","Vietnamese","Indonesian",
#   "Czech","Korean","Ukrainian","Arabic","Greek","Hindi"
# )
# 
# iso2pretty_dt <- data.table(iso = targets_iso,
#                             name = pretty_names)
# 
# # --------------------------------------------------------------
# # 2.  Read the URIEL SYNTACTIC matrix  (≈ 8 000 × 8 000, fast)
# # --------------------------------------------------------------
# syn_file <- file.path(uriel_folder, "SYNTACTIC.csv")
# syn_dt   <- fread(syn_file)       # first column holds row labels
# setnames(syn_dt, 1, "lang1_iso")
# 
# # Retain only languages of interest
# syn_dt <- syn_dt[
#   lang1_iso %in% targets_iso,
#   c("lang1_iso", targets_iso), with = FALSE
# ]
# 
# # --------------------------------------------------------------
# # 3.  Long form: one row per unordered ISO pair
# # --------------------------------------------------------------
# long_dt <- melt(
#   syn_dt,
#   id.vars        = "lang1_iso",
#   variable.name  = "lang2_iso",
#   value.name     = "distance",
#   variable.factor = FALSE
# )[
#   # keep only lines where both are in targets and avoid duplicates
#   lang2_iso %in% targets_iso & lang1_iso < lang2_iso
# ][
#   , metric := "syntactic"
# ]
# 
# # --------------------------------------------------------------
# # 4.  Add pretty language names
# # --------------------------------------------------------------
# setkey(iso2pretty_dt, iso)
# 
# long_dt <- merge(long_dt, iso2pretty_dt,
#                  by.x = "lang1_iso", by.y = "iso", all.x = TRUE)
# setnames(long_dt, "name", "lang1")
# 
# long_dt <- merge(long_dt, iso2pretty_dt,
#                  by.x = "lang2_iso", by.y = "iso", all.x = TRUE)
# setnames(long_dt, "name", "lang2")
# 
# # --------------------------------------------------------------
# # 5.  Force English onto the left (lang1) and keep only
# #     English ↔ target rows, ordered by target name
# # --------------------------------------------------------------
# wide_dt <- long_dt[metric == "syntactic"]   # only one metric anyway
# wide_dt[lang2 == "English", c("lang1","lang2") := .(lang2, lang1)]
# dfL_filt <- wide_dt[
#   lang1 == "English"
# ][order(lang2)]
# 
# # --------------------------------------------------------------
# # 6.  Sanity check: did we get all 21 targets?
# # --------------------------------------------------------------
# missing <- setdiff(pretty_names[-1], dfL_filt$lang2)  # exclude English
# if (length(missing)) {
#   warning("Missing distances for: ", paste(missing, collapse = ", "))
# } else {
#   message("All 21 English‑to‑target distances present ✔︎")
# }
# 
# # --------------------------------------------------------------
# # 7.  Save to CSV  (ready for Figure 1)
# # --------------------------------------------------------------
# fwrite(dfL_filt, out_file)
# 
# # dfL_filt is also left in memory for interactive use
# str(dfL_filt)

dfL_filt <- read_csv("data/lang2vec/processed_syntactic_English_vs_targets.csv")
```


## 2. UK HC
metadata
```{r}
df_UK = read_csv("int_data/UK_processed_health_claims.csv")
```

responses
```{r}
# Load required packages
library(jsonlite)

# 1. List all JSON files in the folder that start with "responses_"
json_files <- list.files(
  path = "int_data/responses_UKHC/",
  pattern = "^responses_.*\\.json$",
  full.names = TRUE
)

# 2. Read each JSON into a data frame
dfs_UK <- lapply(json_files, function(file) {
  # Assumes each JSON file is an array of records (list of objects)
  fromJSON(file, flatten = TRUE)
})

# 3. Combine all data frames into one
df_llm_UK <- bind_rows(dfs_UK)
rm(dfs_UK)
```


```{r}
setDT(df_llm_UK)
# 1. Split off the model and iteration (by “__”)
df_llm_UK[, c("prefix", "model", "iteration") := tstrsplit(custom_id, "__", fixed = TRUE)]

# 2. Split the prefix into the “claim” literal, claim_id, and language (by “_”)
df_llm_UK[, c("dummy", "claim_id", "language") := tstrsplit(prefix, "_", fixed = TRUE)]

# 3. Clean up helper columns and convert iteration to integer
df_llm_UK[, `:=`(
  prefix = NULL,
  dummy = NULL,
  iteration = as.integer(iteration)
)]
```

```{r}
old_status_name <- "Status (Note: Asterisked claims (*) were authorised on the basis of proprietary data and are also listed in the Annex to GB Nutrition and Health Claims Register)"
setnames(df_UK, old = old_status_name, new = "status")

setDT(df_UK)
# 2. Keep only claim_id and status, and make sure claim_id is character to match df_llm
df_UK <- df_UK[, .(
  claim_id = as.character(claim_id),
  status
)]

# 3. Left‑join onto df_llm by claim_id, adding `status`
#    This will add NA where no match is found.
df_llm_UK <- merge(
  x = df_llm_UK,
  y = df_UK,
  by = "claim_id",
  all.x = TRUE
)

df_llm_UK[,status:=ifelse(status=="Authorised for use in Great Britain*","Authorised for use in Great Britain", status)]
df_llm_UK[,status:=ifelse(status=="Authorised for use in Great Britain","Authorised", status)]
```


## 3. EU HC
metadata
```{r}
df_EU = read_csv("int_data/EU_processed_health_claims.csv")

```

responses
```{r}
# Load required packages
library(jsonlite)

# 1. List all JSON files in the folder that start with "responses_"
json_files_EU <- list.files(
  path = "int_data/responses_EUHC/",
  pattern = "^responses_.*\\.json$",
  full.names = TRUE
)

# 2. Read each JSON into a data frame
dfs_EU <- lapply(json_files_EU, function(file) {
  # Assumes each JSON file is an array of records (list of objects)
  fromJSON(file, flatten = TRUE)
})

# 3. Combine all data frames into one
df_llm_EU <- bind_rows(dfs_EU)
rm(dfs_EU)
```

```{r}
setDT(df_llm_EU)
# 1. Split off the model and iteration (by “__”)
df_llm_EU[, c("prefix", "model", "iteration") := tstrsplit(custom_id, "__", fixed = TRUE)]

# 2. Split the prefix into the “claim” literal, claim_id, and language (by “_”)
df_llm_EU[, c("dummy", "claim_id", "language") := tstrsplit(prefix, "_", fixed = TRUE)]

# 3. Clean up helper columns and convert iteration to integer
df_llm_EU[, `:=`(
  prefix = NULL,
  dummy = NULL,
  iteration = as.integer(iteration)
)]
```

```{r}
library(data.table)

# Ensure both are data.tables
setDT(df_llm_EU)
setDT(df_EU)

# 1. Rename the long Status column in `df` to just "status"
#    Replace the old name below with your exact column name.
old_status_name <- "Status"
setnames(df_EU, old = old_status_name, new = "status")

setDT(df_EU)
# 2. Keep only claim_id and status, and make sure claim_id is character to match df_llm
df_EU <- df_EU[, .(
  claim_id = as.character(claim_id),
  status
)]

# 3. Left‑join onto df_llm by claim_id, adding `status`
#    This will add NA where no match is found.
df_llm_EU <- merge(
  x = df_llm_EU,
  y = df_EU,
  by = "claim_id",
  all.x = TRUE
)

df_llm_EU[, .N, by = status]

```


## merge EU, uk

```{r}

df_llm_EU %<>% mutate(jurisfication="EU")
df_llm_EU %<>% mutate(custom_id=paste0("EU", custom_id))
glimpse(df_llm_EU)
df_llm_UK %<>% mutate(jurisfication="UK")
df_llm_UK %<>% mutate(custom_id=paste0("UK", custom_id))

df_llm<-bind_rows(df_llm_EU,df_llm_UK)
rm(df_llm_EU,df_llm_UK)
gc()
```


# preprocess

```{r}
df_llm[,claim_id:=paste0(jurisfication,claim_id)]

```

clean output
```{r}
df_llm %<>% rename(response_raw=response)


df_llm[, response := {
  # Helper: for a single string, extract all '0' or '1' characters and return the last as integer
  extract_last01 <- function(txt) {
    # find all matches of the characters '0' or '1'
    matches <- regmatches(txt, gregexpr("[01]", txt))[[1]]
    if (length(matches) > 0) {
      return(as.integer(tail(matches, 1)))
    } else {
      return(NA_integer_)
    }
  }
  # Vectorise over the column
  vapply(response_raw, extract_last01, integer(1))
}]

# Check the distribution
a<-df_llm[, .N, by = response][order(response)]

df_llm[,response:=ifelse(response>1,NA,response)]

```


true label
```{r}
df_llm[,label:=1]
df_llm$response <- as.numeric(df_llm$response)
df_llm$iteration <- as.numeric(df_llm$iteration)

df_llm[,.N,by=model]
```

clean: remove NAs
```{r}
df_llm[iteration<4,
  .(  
    total     = .N,  
    missing   = sum(is.na(response)),  
    non_missing = sum(!is.na(response))  
  ), by = model]
```

# evaluation


```{r}
setDT(df_llm)
df_modal <- df_llm[, .(
  modal_response = {
    tb <- table(response)
    as.integer(names(tb)[which.max(tb)])
  },
  label  = label[1],
  status = status[1]
), by = .(claim_id, language, model)]

# Evaluation by model, language, and status
evaluation_lang_status <- df_modal[!is.na(modal_response)][!is.na(status)][model!="phi3.5:latest"][, .(
  TruePos   = sum(modal_response == 1 & label == 1),
  FalseNeg  = sum(modal_response == 0 & label == 1),
  Accuracy  = sum(modal_response == 1 & label == 1) / .N
), by = .(model, language, status)]



# Filter out unwanted models & statuses
eval_lang_filt <- evaluation_lang_status %>%
  filter(
    status == "Authorised"
    )

# Mean accuracy per language (averaging over models & status)
accuracy_lang <- eval_lang_filt %>%
  group_by(language) %>%
  summarize(Accuracy = mean(Accuracy, na.rm = TRUE), .groups = "drop")

# Prepare Levenshtein distance table: English ↔ other
distances <- dfL_filt %>%
  select(lang1, lang2, distance) %>%
  mutate(
    other = if_else(lang2 == "ENGLISH", lang2, lang1)
  ) %>%
  select(language = other, distance) %>%
  distinct() %>%
  mutate(
    language = recode(language,
      STANDARD_GERMAN = "German",
      MIDDLE_CHINESE  = "Chinese",
      STANDARD_ARABIC = "Arabic"
    ),
    language = str_to_title(tolower(language))
  )

# Normalize accuracy_lang language names and join distances
plot_df <- accuracy_lang %>%
  mutate(language = str_to_title(tolower(language))) %>%
  left_join(distances, by = "language")

#  Compute per‐model slopes and mean accuracies
#    a) accuracy per model & language
accuracy_model_lang <- eval_lang_filt %>%
  group_by(model, language) %>%
  summarize(Accuracy = mean(Accuracy, na.rm = TRUE), .groups = "drop") %>%
  mutate(language = str_to_title(tolower(language)))

#    b) join to distances
model_dist_df <- accuracy_model_lang %>%
  left_join(distances, by = "language")

model_stats <- model_dist_df %>%
  mutate(
    model_clean = model %>%
      str_remove_all("-\\d{4}-\\d{2}-\\d{2}") %>%
      str_remove_all(":latest") %>%
      str_replace_all("gemini-2\\.0-flash-lite", "gemini-2-flash")
  ) %>%
  group_by(model_clean) %>%
  summarize(
    mean_acc = mean(Accuracy, na.rm = TRUE),
    slope    = coef(lm(Accuracy ~ distance, data = cur_data()))[2],
    .groups = "drop"
  ) %>%
  # pad positive slopes so they line up under minus signs
  mutate(
    slope_str = ifelse(
      slope < 0,
      sprintf("<span style='color:black'>%.2f</span>", slope),
      sprintf("&nbsp;%.2f", slope)
    ),
    label = sprintf(
      "**%s**: mean=%.2f; slope=%s",
      model_clean, mean_acc, slope_str
    )
  ) %>%
  # **here** reorder by slope ascending (most negative first)
  arrange(slope)

# then collapse in that order:
label_block <- paste(model_stats$label, collapse = "<br>")


# precompute the data‐range
xmin <- min(plot_df$distance,   na.rm = TRUE)
ymin <- min(plot_df$Accuracy,   na.rm = TRUE)


library(ggtext)  # for rich text annotations
# Draw the plot with rich‐text annotation in bottom‐left
ggplot(plot_df, aes(x = distance, y = Accuracy)) +
  geom_point(size = 4) +
  geom_text_repel(aes(label = language), max.overlaps = 20, size = 6) +
  geom_richtext(
    data = tibble(x = xmin + 0.22, y = ymin+0.007),
    aes(x = x, y = y, label = label_block),
    fill       = NA,
    label.color= NA,
    hjust      = 1,
    vjust      = 0,
    size       = 6,
    family     = "mono"
  ) +
  labs(
    title = "",
    x     = "Levenshtein Distance to English",
    y     = "Mean Accuracy"
  ) +
  theme_classic(base_size = 20) +
  theme(
    legend.position   = "none",
    axis.text.x       = element_text(angle = 45, hjust = 1),
    plot.title        = element_blank()
  )


ggsave(plot=last_plot(), filename="figures/fig1.jpg",width = 12,height = 8,dpi=300)
ggsave(plot=last_plot(), filename="figures/fig1.pdf",width = 12,height = 8,dpi=300)
```


```{r}
###############################################################################
## Figure 1  – Mean accuracy (95 % Wilson CI) vs. syntactic distance to English
###############################################################################

library(data.table)
library(tidyverse)
library(binom)
library(ggrepel)
library(ggtext)

setDT(df_llm)

## ────────────────────────────────────────────────────────────────
## 1.  Modal response per claim / language / model
## ────────────────────────────────────────────────────────────────
df_modal <- df_llm[, .(
  modal_response = {
    tb <- table(response)
    as.integer(names(tb)[which.max(tb)])
  },
  label  = label[1],
  status = status[1]
), by = .(claim_id, language, model)]

## ────────────────────────────────────────────────────────────────
## 2.  Evaluation (Authorised only, drop phi3.5 stub)
## ────────────────────────────────────────────────────────────────
evaluation_lang_status <- df_modal[
  !is.na(modal_response) & !is.na(status) &
    model != "phi3.5:latest",
  .(
    TruePos   = sum(modal_response == 1 & label == 1),
    FalseNeg  = sum(modal_response == 0 & label == 1)
  ),
  by = .(model, language, status)
]

eval_lang_filt <- evaluation_lang_status[status == "Authorised"]

## ────────────────────────────────────────────────────────────────
## 3.  Mean accuracy + Wilson CI per language
## ────────────────────────────────────────────────────────────────
lang_totals <- eval_lang_filt[ , .(
  TP = sum(TruePos),
  FN = sum(FalseNeg)
), by = language][ , N := TP + FN ][ , Accuracy := TP / N ]

ci_tbl <- binom.confint(lang_totals$TP, lang_totals$N, method = "wilson")
lang_totals[ , `:=`(CI_low = ci_tbl$lower, CI_high = ci_tbl$upper) ]
lang_totals[ , language := str_to_title(tolower(language)) ]

## ────────────────────────────────────────────────────────────────
## 4.  Syntactic‑distance look‑up  (dfL_filt already built earlier)
## ────────────────────────────────────────────────────────────────
distances <- dfL_filt %>%        # columns: lang1, lang2, distance_syntactic
  transmute(language = str_to_title(tolower(lang2)), distance)

plot_df <- left_join(lang_totals, distances, by = "language")

## ────────────────────────────────────────────────────────────────
## 5.  Per‑model annotation block (mean & slope)
## ────────────────────────────────────────────────────────────────
accuracy_model_lang <- eval_lang_filt %>%
  group_by(model, language) %>%
  summarise(Accuracy = TruePos / (TruePos + FalseNeg), .groups = "drop") %>%
  mutate(language = str_to_title(tolower(language))) %>%
  left_join(distances, by = "language")

model_stats <- accuracy_model_lang %>% 
  mutate(model_clean = model %>% 
           str_remove_all("-\\d{4}-\\d{2}-\\d{2}|:latest") %>% 
           str_replace_all("gemini-2\\.0-flash-lite", "gemini-2-flash")) %>% 
  group_by(model_clean) %>% 
  summarise(
    mean_acc = mean(Accuracy, na.rm = TRUE),
    slope    = coef(lm(Accuracy ~ distance, data = cur_data()))[2],  # ← cur_data()
    .groups  = "drop"
  ) %>% 
  arrange(slope) %>% 
  mutate(label = sprintf("**%s**: mean=%.2f; slope=%.2f",
                         model_clean, mean_acc, slope))


label_block <- paste(model_stats$label, collapse = "<br>")

## ────────────────────────────────────────────────────────────────
## 6.  Plot
## ────────────────────────────────────────────────────────────────
xmin <- min(plot_df$distance, na.rm = TRUE)
ymin <- min(plot_df$Accuracy, na.rm = TRUE)

p <- ggplot(plot_df %>% drop_na(distance), 
            aes(distance, Accuracy)) +
  geom_errorbar(aes(ymin = CI_low, ymax = CI_high),
                width = 0.003, alpha = 0.6, linewidth = 0.6) +
  geom_point(size = 4,  alpha = 0.6) +
  geom_text_repel(aes(label = language),
                  max.overlaps = 20, size = 6) +
  ## ——— fixed‑position annotation ———
  geom_richtext(
    data = tibble(x = 0.60, y = 0.855, label = label_block),
    aes(x, y, label = label),
    inherit.aes = FALSE,
    fill        = NA,
    label.color = NA,
    hjust       = 1,
    vjust       = 0,
    size        = 6,
    family      = "mono"
  ) +
  ## ——— same fixed viewport as the earlier version ———
  coord_cartesian(xlim = c(0.37, 0.75),
                  ylim = c(0.85, 1.00)) +
  labs(x = "Syntactic distance from English",
       y = "Mean accuracy (95 % Wilson CI)") +
  theme_classic(base_size = 20) +
  theme(axis.text.x     = element_text(angle = 45, hjust = 1),
        legend.position = "none")


ggsave(plot = p, filename = "figures/fig1.jpg",
       width = 12, height = 8, dpi = 300)
ggsave(plot = p, filename = "figures/fig1.pdf",
       width = 12, height = 8, dpi = 300)

```

